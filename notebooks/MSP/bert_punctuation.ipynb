{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krjohnn/LU-VTL-25m/blob/main/notebooks/MSP/bert_punctuation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c1f7fc1",
      "metadata": {
        "id": "3c1f7fc1"
      },
      "source": [
        "# Comma Restoration with Token Classification using BERT\n",
        "This notebook fine-tunes a transformer encoder (e.g., mBERT, LvBERT) to restore commas in text as a token classification task. Each token receives one of two labels: COMMA (a comma should follow this word) or O (no comma). At inference time, existing commas are stripped, labels are predicted, and the sentence is rebuilt by inserting commas after tokens predicted as COMMA.\n",
        "Models to try:\n",
        "- https://huggingface.co/google-bert/bert-base-multilingual-cased\n",
        "- https://huggingface.co/AiLab-IMCS-UL/lvbert\n",
        "- https://huggingface.co/FacebookAI/xlm-roberta-base\n",
        "- https://huggingface.co/EMBEDDIA/litlat-bert\n",
        "- https://huggingface.co/jhu-clsp/mmBERT-small"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1891982",
      "metadata": {
        "id": "f1891982"
      },
      "source": [
        "# Prepare environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5JF6qA_UuX3s",
      "metadata": {
        "id": "5JF6qA_UuX3s"
      },
      "outputs": [],
      "source": [
        "# Authenticate with Weights & Biases to enable logging and experiment tracking.\n",
        "# Comment out the following lines if you don't want to use W&B.\n",
        "!pip install wandb\n",
        "import wandb\n",
        "import os\n",
        "from google.colab import userdata\n",
        "# Get your 86-character key from secrets\n",
        "api_key = userdata.get('WANDB_API_KEY')\n",
        "\n",
        "# Set the environment variable manually as a backup\n",
        "os.environ[\"WANDB_API_KEY\"] = api_key\n",
        "\n",
        "# Login (relogin=True helps clear out any old 40-character errors)\n",
        "wandb.login(key=api_key, relogin=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H872W0AqxhKQ",
      "metadata": {
        "id": "H872W0AqxhKQ"
      },
      "outputs": [],
      "source": [
        "# Check if a CUDA device is available\n",
        "!pip install torch\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print('CUDA device:', torch.cuda.get_device_name(0), torch.cuda.get_device_capability(0), 'bf16', torch.cuda.is_bf16_supported(False))\n",
        "    free_mem, total_mem = torch.cuda.mem_get_info(torch.device('cuda:0'))\n",
        "    print(f'Memory: {free_mem / 1024 ** 2:.2f} MB free / {total_mem / 1024 ** 2:.2f} MB total')\n",
        "else:\n",
        "    print('No CUDA device available')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68105191",
      "metadata": {
        "id": "68105191"
      },
      "outputs": [],
      "source": [
        "!python -V\n",
        "!pip -V\n",
        "!pip install numpy transformers[torch] scikit-learn datasets wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a1fb23e",
      "metadata": {
        "id": "5a1fb23e"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import requests\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    DataCollatorForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    set_seed,\n",
        ")\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60237262",
      "metadata": {
        "id": "60237262",
        "lines_to_next_cell": 1
      },
      "source": [
        "# Prepare dataset\n",
        "Raw sentences from the Latvian Universal Dependencies (LVTB) corpus: https://universaldependencies.org/treebanks/lv_lvtb/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb6ac0fa",
      "metadata": {
        "id": "bb6ac0fa"
      },
      "outputs": [],
      "source": [
        "def fetch_ud_texts(split, seed=42):\n",
        "    conllu = requests.get(f'https://raw.githubusercontent.com/UniversalDependencies/UD_Latvian-LVTB/r2.16/lv_lvtb-ud-{split}.conllu').text\n",
        "    texts = [line[9:].strip() for line in conllu.splitlines() if line.startswith('# text = ')]\n",
        "    if seed:\n",
        "        import random\n",
        "        random.Random(seed).shuffle(texts)\n",
        "    return texts\n",
        "\n",
        "def prepare_data(max_chars=200, dev_txt='dev.txt', train_txt='train.txt'):\n",
        "    # Download UD Latvian splits, filter by mBERT token count, and save plain .txt files.\n",
        "    dev_texts = fetch_ud_texts('dev')\n",
        "    train_texts = fetch_ud_texts('train')\n",
        "\n",
        "    if max_chars:\n",
        "        # Filter out long sentences to avoid truncation\n",
        "        print('Sentence lengths before filtering:', 'DEV', len(dev_texts), 'TRAIN', len(train_texts))\n",
        "        dev_texts = [t for t in dev_texts if len(t) <= max_chars]\n",
        "        train_texts = [t for t in train_texts if len(t) <= max_chars]\n",
        "    print('Dataset sentence lengths:', 'DEV', len(dev_texts), 'TRAIN', len(train_texts))\n",
        "\n",
        "    with open(dev_txt, 'w') as f:\n",
        "        for t in dev_texts: f.write(t + '\\n')\n",
        "    with open(train_txt, 'w') as f:\n",
        "        for t in train_texts: f.write(t + '\\n')\n",
        "\n",
        "    return dev_texts, train_texts\n",
        "\n",
        "dev_texts, train_texts = prepare_data()\n",
        "print(*train_texts[:5], sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c18df7d8",
      "metadata": {
        "id": "c18df7d8"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b87d7f30",
      "metadata": {
        "id": "b87d7f30"
      },
      "outputs": [],
      "source": [
        "def tokenize(s):\n",
        "    # Tokenizes string into words and punctuation tokens.\n",
        "    return re.findall(r'\\s*(?:\\w+|\\S)', s)\n",
        "\n",
        "def tokenize_with_comma_labels(s):\n",
        "    tokens_with_labels = re.findall(r'(\\s*\\w+|[^\\s,])\\s*(,+)?', s)\n",
        "    tokens_with_labels = [(tok, 'COMMA' if comma else 'O') for tok, comma in tokens_with_labels]\n",
        "    return tokens_with_labels\n",
        "\n",
        "def remove_commas(s) -> str:\n",
        "    return re.sub(r'\\s*,+\\s*', ' ', s)\n",
        "\n",
        "tokenize_with_comma_labels('Vēl 9% sacīja, ka nav izlēmuši kā balsot, bet 3,2% atteicās atbildēt.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5f2d561",
      "metadata": {
        "id": "a5f2d561"
      },
      "outputs": [],
      "source": [
        "def test_tokenization(model=None):\n",
        "    s = 'Vēl 9% sacīja, ka nav izlēmuši kā balsot, bet 3,2% atteicās atbildēt.'\n",
        "    if model:\n",
        "        print('Tokenizer stats', model)\n",
        "        t = AutoTokenizer.from_pretrained(model)\n",
        "        print('Encoded sample:', t(s))\n",
        "        print('Encoded sample - subword units:', t.convert_ids_to_tokens(t.encode(s)))\n",
        "        lengths = sorted([len(t.encode(seq)) for seq in train_texts])\n",
        "        print(f'Max {max(lengths)}, min {min(lengths)}, avg {sum(lengths)/len(lengths)}')\n",
        "        print(f'95% length: {lengths[int(len(lengths) * 0.95)]}')\n",
        "        print(f'99% length: {lengths[int(len(lengths) * 0.99)]}')\n",
        "        print(f'99.9% length: {lengths[int(len(lengths) * 0.999)]}')\n",
        "\n",
        "test_tokenization('AiLab-IMCS-UL/lvbert')\n",
        "test_tokenization('jhu-clsp/mmBERT-small')\n",
        "\n",
        "\n",
        "LABELS = ['O', 'COMMA']\n",
        "LABEL2ID = {name: i for i, name in enumerate(LABELS)}\n",
        "ID2LABEL = {i: name for i, name in enumerate(LABELS)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e61dbedb",
      "metadata": {
        "id": "e61dbedb"
      },
      "outputs": [],
      "source": [
        "# Tokenize text into subwords and align word-level labels to the correct subword positions\n",
        "\n",
        "def tokenize_and_align_labels(tokenizer, words, word_labels=None, label2id=None, debug=False, return_tensors=None):\n",
        "    # Tokenize with word boundaries preserved\n",
        "    enc = tokenizer(\n",
        "        list(words),\n",
        "        is_split_into_words=True,\n",
        "        add_special_tokens=True,\n",
        "        return_tensors=return_tensors,\n",
        "        truncation=False\n",
        "    )\n",
        "\n",
        "    # Map each token back to its source word index\n",
        "    word_ids = enc.word_ids()  # one per token position (None for specials)\n",
        "\n",
        "    # Figure out which token is the last subword of each word. Assign word labels only there; others get -100.\n",
        "    # HF Trainer and loss functions (like cross-entropy) automatically ignore -100, so you don't need to modify loss computation.\n",
        "    if word_labels is not None:\n",
        "        IGNORE = -100\n",
        "        labels = [IGNORE] * len(word_ids)\n",
        "        for i, wid in enumerate(word_ids):\n",
        "            if wid is None:\n",
        "                continue\n",
        "            next_wid = word_ids[i+1] if i+1 < len(word_ids) else None\n",
        "            if wid != next_wid:\n",
        "                # last subword of this word: assign the word label\n",
        "                labels[i] = label2id[word_labels[wid]]\n",
        "    else:\n",
        "        labels = None\n",
        "\n",
        "    if debug:\n",
        "        input_ids = enc['input_ids']\n",
        "        if return_tensors == 'pt':\n",
        "            input_ids = input_ids.tolist()[0]\n",
        "        print('WORDS:         ', words)\n",
        "        print('WORD_LABELS:   ', word_labels)\n",
        "        print('WORD_IDS:      ', word_ids)\n",
        "        print('TOKEN_IDS:     ', input_ids)\n",
        "        print('TOKENS:        ', tokenizer.convert_ids_to_tokens(input_ids))\n",
        "        print('ALIGNED_LABELS:', labels)\n",
        "\n",
        "    r = {\n",
        "        'input_ids': enc['input_ids'],\n",
        "        'attention_mask': enc['attention_mask'],\n",
        "    }\n",
        "    if labels is not None:\n",
        "        if return_tensors == 'pt':\n",
        "            labels = torch.tensor([labels], dtype=torch.long)\n",
        "        r['labels'] = labels\n",
        "    return r\n",
        "\n",
        "print(tokenize_and_align_labels(AutoTokenizer.from_pretrained('AiLab-IMCS-UL/lvbert'), *zip(*tokenize_with_comma_labels('Viens, divi.')), LABEL2ID, debug=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L7mOk3y4v9Wc",
      "metadata": {
        "id": "L7mOk3y4v9Wc"
      },
      "source": [
        "# Tokenize and format dataset for model training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bd737c4",
      "metadata": {
        "id": "4bd737c4"
      },
      "outputs": [],
      "source": [
        "def build_dataset(*, tokenizer, train_file='train.txt', dev_file='dev.txt', train_samples=None, dev_samples=None, max_length=100, label2id=None):\n",
        "    ds = load_dataset('text', data_files={'train': train_file, 'dev': dev_file})\n",
        "    if train_samples:\n",
        "        ds['train'] = ds['train'].take(train_samples)\n",
        "    if dev_samples:\n",
        "        ds['dev'] = ds['dev'].take(dev_samples)\n",
        "\n",
        "    def _map(example):\n",
        "        words, word_labels = zip(*tokenize_with_comma_labels(example['text']))\n",
        "        return tokenize_and_align_labels(tokenizer, words=words, word_labels=word_labels, label2id=label2id)\n",
        "\n",
        "    ds_tokenized = ds.map(_map, remove_columns=ds['train'].column_names)\n",
        "\n",
        "    if max_length is not None:\n",
        "        ds_tokenized = ds_tokenized.filter(lambda ex: len(ex['input_ids']) <= max_length)\n",
        "\n",
        "    return ds_tokenized\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained('AiLab-IMCS-UL/lvbert')\n",
        "ds = build_dataset(tokenizer=tok, train_samples=2, dev_samples=2, label2id=LABEL2ID)\n",
        "loader = DataLoader(ds['train'], batch_size=2, shuffle=False, collate_fn=DataCollatorForTokenClassification(tok))\n",
        "batch = next(iter(loader))\n",
        "print(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f0e1fe7",
      "metadata": {
        "id": "9f0e1fe7"
      },
      "source": [
        "# Metrics for token classification.\n",
        "*Accuracy* can be misleading for imbalanced tasks:\n",
        "  - In our data, most tokens are \"O\" (no comma).\n",
        "  - A dumb model that always predicts \"O\" could reach very high accuracy (e.g. 95%+) simply by never predicting commas at all.\n",
        "\n",
        "*F1-score* (the harmonic mean of precision and recall) specifically for the COMMA class gives a more honest view of model quality:\n",
        "  - Precision: when the model predicts COMMA, is it right?\n",
        "  - Recall: does the model catch most of the true commas?\n",
        "  - F1: balances both, penalizing if one is much lower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baa2fe37",
      "metadata": {
        "id": "baa2fe37"
      },
      "outputs": [],
      "source": [
        "def compute_metrics_fn(p):\n",
        "    # Model outputs: shape [batch_size, seq_len, num_labels]\n",
        "    # -> pick the most likely label for each token\n",
        "    preds = np.argmax(p.predictions, axis=-1)\n",
        "\n",
        "    # True labels: shape [batch_size, seq_len]\n",
        "    labels = p.label_ids\n",
        "\n",
        "    # Flatten but skip positions marked with -100\n",
        "    y_ref = []\n",
        "    y_pred = []\n",
        "    for ref_seq, pred_seq in zip(labels, preds):\n",
        "        for t, p_ in zip(ref_seq, pred_seq):\n",
        "            if t == -100:\n",
        "                continue\n",
        "            y_ref.append(t)\n",
        "            y_pred.append(p_)\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        y_ref, y_pred,\n",
        "        average='binary', pos_label=1, # for binary classification (COMMA vs O)\n",
        "        # average='micro', # for multi-class classification\n",
        "    )\n",
        "    acc = accuracy_score(y_ref, y_pred)\n",
        "    return {\n",
        "        'f1': f1,\n",
        "        'p': precision,\n",
        "        'r': recall,\n",
        "        'acc': acc,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ee51e8a",
      "metadata": {
        "id": "5ee51e8a"
      },
      "source": [
        "# Inference\n",
        "Given plain text, we strip commas, tokenize with word boundaries, run the model, and insert commas after tokens labeled COMMA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8013c61a",
      "metadata": {
        "id": "8013c61a"
      },
      "outputs": [],
      "source": [
        "def process_text(text, model, tokenizer, verbose=True):\n",
        "    #  Preprocess: remove commas, split into words\n",
        "    input_text = remove_commas(text)\n",
        "\n",
        "    # Tokenize with subword alignment\n",
        "    words = tokenize(input_text)\n",
        "    enc = tokenizer(\n",
        "        words,\n",
        "        is_split_into_words=True,\n",
        "        add_special_tokens=True,\n",
        "        return_tensors='pt',\n",
        "        truncation=False\n",
        "    )\n",
        "    word_ids = enc.word_ids()\n",
        "    # Move to the same device\n",
        "    device = next(model.parameters()).device\n",
        "    enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "    # Forward pass\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(**enc).logits  # [1, seq_len, label_count]\n",
        "        pred_ids = torch.argmax(logits, dim=-1).squeeze(0).tolist()\n",
        "\n",
        "    # Collapse subwords -> last subword gets the label\n",
        "    word_preds = {}\n",
        "    for i, wid in enumerate(word_ids):\n",
        "        if wid is None:  # skip [CLS], [SEP], etc.\n",
        "            continue\n",
        "        next_wid = word_ids[i + 1] if i + 1 < len(word_ids) else None\n",
        "        if wid != next_wid:  # last subword of the word\n",
        "            word_preds[wid] = pred_ids[i]\n",
        "\n",
        "    # Return word-level predictions\n",
        "    results = [(w, model.config.id2label[word_preds[i]]) for i, w in enumerate(words)]\n",
        "    output_text = ''.join([w + (',' if label == 'COMMA' else '') for w, label in results])\n",
        "\n",
        "    if verbose:\n",
        "        print(f'REF: {text}')\n",
        "        print(f' IN: {input_text}')\n",
        "        print(f'OUT: {output_text}')\n",
        "    return output_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8add874a",
      "metadata": {
        "id": "8add874a"
      },
      "source": [
        "# Model fine-tuning\n",
        "- Track loss curves, gradient norms, and evaluation metrics over time\n",
        "- Use an appropriate optimizer and learning rate schedule (e.g., warmup + decay)\n",
        "- Watch for overfitting (gap between train and eval performance)\n",
        "- Adjust batch size, accumulation steps, or precision (fp16/bf16) if needed\n",
        "- Save best checkpoints based on validation metric (e.g., F1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcb5e6f8",
      "metadata": {
        "id": "dcb5e6f8",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def main(\n",
        "    name,\n",
        "    base_model='AiLab-IMCS-UL/lvbert',\n",
        "    max_len=100,\n",
        "    seed=42,\n",
        "    verbose=True,\n",
        "    lr=5e-6,\n",
        "    bs=32,\n",
        "    train_samples=None,\n",
        "    dev_samples=None,\n",
        "    epochs=3,\n",
        "    report_wandb=True,\n",
        "    wandb_group=None,\n",
        "    save=True,\n",
        "):\n",
        "    if report_wandb and not wandb.api.api_key:\n",
        "        print('Not authenticated with W&B')\n",
        "        report_wandb = False\n",
        "\n",
        "    with wandb.init(project='punctuator', group=wandb_group, name=name) if report_wandb else nullcontext():\n",
        "        print('Train:', locals())\n",
        "        set_seed(seed)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "\n",
        "        ds = build_dataset(tokenizer=tokenizer, train_samples=train_samples, dev_samples=dev_samples, max_length=max_len, label2id=LABEL2ID)\n",
        "\n",
        "        # Initialize base model for token classification task\n",
        "        model = AutoModelForTokenClassification.from_pretrained(base_model, num_labels=len(LABELS), id2label=ID2LABEL, label2id=LABEL2ID)\n",
        "        model.config.use_cache = False\n",
        "\n",
        "        # Define training hyperparameters\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=name,\n",
        "            learning_rate=lr,\n",
        "            per_device_train_batch_size=bs,\n",
        "            per_device_eval_batch_size=bs,\n",
        "            num_train_epochs=epochs,\n",
        "            eval_strategy='epoch',\n",
        "            save_strategy='epoch' if save else 'no',\n",
        "            load_best_model_at_end=save,\n",
        "            metric_for_best_model='f1',\n",
        "            greater_is_better=True,\n",
        "            warmup_ratio=0.05,\n",
        "            gradient_accumulation_steps=1,\n",
        "            fp16=True,\n",
        "\n",
        "            logging_steps=20,\n",
        "            report_to='wandb' if report_wandb else 'none',\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=ds['train'],\n",
        "            eval_dataset=ds['dev'],\n",
        "            processing_class=tokenizer,\n",
        "            data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer),\n",
        "            compute_metrics=compute_metrics_fn,\n",
        "        )\n",
        "\n",
        "        # Actual training\n",
        "        trainer.train()\n",
        "        if save:\n",
        "            trainer.save_model(name)\n",
        "            tokenizer.save_pretrained(name)\n",
        "\n",
        "        process_text('Vēl 9% sacīja, ka nav izlēmuši kā balsot, bet 3,2% atteicās atbildēt.', trainer.model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W6a4_ozqlhu9",
      "metadata": {
        "id": "W6a4_ozqlhu9"
      },
      "outputs": [],
      "source": [
        "# Use only 3000 samples for training to run a quick experiment\n",
        "main('bert_punctuator_sample', train_samples=3000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0ad81c5",
      "metadata": {
        "id": "b0ad81c5"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd5b0ecb",
      "metadata": {
        "id": "cd5b0ecb"
      },
      "outputs": [],
      "source": [
        "m = AutoModelForTokenClassification.from_pretrained('bert_punctuator_sample')\n",
        "t = AutoTokenizer.from_pretrained('bert_punctuator_sample')\n",
        "process_text('Vēl 9% sacīja, ka nav izlēmuši kā balsot, bet 3,2% atteicās atbildēt.', m, t)\n",
        "process_text('Nogalināt nedrīkst, apžēlot!', m, t)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1be43210",
      "metadata": {
        "id": "1be43210"
      },
      "source": [
        "# Hyperparameter optimization\n",
        "- Use smaller experiments (1 epoch, limited data) for faster iteration  \n",
        "- Try random or Bayesian search for hyperparameter tuning\n",
        "- Limit training/eval samples when testing setups  \n",
        "- Scale up once the pipeline works end-to-end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f158725",
      "metadata": {
        "id": "8f158725",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "for lr in [1e-6, 3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3]:\n",
        "    main(f'bert_sweep_lr{lr:.2e}', lr=lr, train_samples=3000, dev_samples=100, epochs=1, wandb_group='bert_sweep', save=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "cell_metadata_filter": "colab,id,outputId,-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}