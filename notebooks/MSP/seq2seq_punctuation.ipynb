{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krjohnn/LU-VTL-25m/blob/main/notebooks/MSP/seq2seq_punctuation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01470473",
      "metadata": {
        "id": "01470473"
      },
      "source": [
        "# Comma Restoration with Encoder-Decoder Models (Seq2Seq)\n",
        "This notebook fine-tunes sequence-to-sequence transformers to restore commas by generating the punctuated sentence from an unpunctuated input. During training, commas are removed from the source text; the target is the original, correctly punctuated sentence. At inference, the model takes clean, comma-stripped text and outputs the version with commas inserted end-to-end—no token-level labels needed.\n",
        "\n",
        "Models to try:\n",
        "- https://huggingface.co/google/mt5-small\n",
        "- https://huggingface.co/google/byt5-small"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c8dc197",
      "metadata": {
        "id": "4c8dc197"
      },
      "source": [
        "# Prepare environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sD7SnreC22k9",
      "metadata": {
        "id": "sD7SnreC22k9"
      },
      "outputs": [],
      "source": [
        "# Authenticate with Weights & Biases to enable logging and experiment tracking.\n",
        "# Comment out the following lines if you don't want to use W&B.\n",
        "!pip install wandb\n",
        "import wandb\n",
        "import os\n",
        "from google.colab import userdata\n",
        "# Get your 86-character key from secrets\n",
        "api_key = userdata.get('WANDB_API_KEY')\n",
        "\n",
        "# Set the environment variable manually as a backup\n",
        "os.environ[\"WANDB_API_KEY\"] = api_key\n",
        "\n",
        "# Login (relogin=True helps clear out any old 40-character errors)\n",
        "wandb.login(key=api_key, relogin=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2ddf0fa",
      "metadata": {
        "id": "e2ddf0fa"
      },
      "outputs": [],
      "source": [
        "# Check if a CUDA device is available\n",
        "!pip install torch\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print('CUDA device:', torch.cuda.get_device_name(0), torch.cuda.get_device_capability(0), 'bf16', torch.cuda.is_bf16_supported(False))\n",
        "    free_mem, total_mem = torch.cuda.mem_get_info(torch.device('cuda:0'))\n",
        "    print(f'Memory: {free_mem / 1024 ** 2:.2f} MB free / {total_mem / 1024 ** 2:.2f} MB total')\n",
        "else:\n",
        "    print('No CUDA device available')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34b6fe0c",
      "metadata": {
        "id": "34b6fe0c"
      },
      "outputs": [],
      "source": [
        "!python -V\n",
        "!pip -V\n",
        "!pip install numpy transformers[torch] scikit-learn datasets wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2651c6be",
      "metadata": {
        "id": "2651c6be"
      },
      "outputs": [],
      "source": [
        "import difflib\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import requests\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from contextlib import nullcontext\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    set_seed, PreTrainedTokenizerBase, Seq2SeqTrainer,\n",
        ")\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0f5e365",
      "metadata": {
        "id": "f0f5e365",
        "lines_to_next_cell": 1
      },
      "source": [
        "# Prepare dataset\n",
        "Raw sentences from the Latvian Universal Dependencies (LVTB) corpus: https://universaldependencies.org/treebanks/lv_lvtb/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2795b03c",
      "metadata": {
        "id": "2795b03c",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def fetch_ud_texts(split, seed=42):\n",
        "    conllu = requests.get(f'https://raw.githubusercontent.com/UniversalDependencies/UD_Latvian-LVTB/r2.16/lv_lvtb-ud-{split}.conllu').text\n",
        "    texts = [line[9:].strip() for line in conllu.splitlines() if line.startswith('# text = ')]\n",
        "    if seed:\n",
        "        import random\n",
        "        random.Random(seed).shuffle(texts)\n",
        "    return texts\n",
        "\n",
        "def prepare_data(max_chars=200, dev_txt='dev.txt', train_txt='train.txt'):\n",
        "    # Download UD Latvian splits, filter by mBERT token count, and save plain .txt files.\n",
        "    dev_texts = fetch_ud_texts('dev')\n",
        "    train_texts = fetch_ud_texts('train')\n",
        "\n",
        "    if max_chars:\n",
        "        # Filter out long sentences to avoid truncation\n",
        "        print('Sentence lengths before filtering:', 'DEV', len(dev_texts), 'TRAIN', len(train_texts))\n",
        "        dev_texts = [t for t in dev_texts if len(t) <= max_chars]\n",
        "        train_texts = [t for t in train_texts if len(t) <= max_chars]\n",
        "    print('Dataset sentence lengths:', 'DEV', len(dev_texts), 'TRAIN', len(train_texts))\n",
        "\n",
        "    with open(dev_txt, 'w') as f:\n",
        "        for t in dev_texts: f.write(t + '\\n')\n",
        "    with open(train_txt, 'w') as f:\n",
        "        for t in train_texts: f.write(t + '\\n')\n",
        "\n",
        "    return dev_texts, train_texts\n",
        "\n",
        "dev_texts, train_texts = prepare_data()\n",
        "print(*train_texts[:5], sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b19d374",
      "metadata": {
        "id": "4b19d374"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60e4c142",
      "metadata": {
        "id": "60e4c142",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# def remove_commas(s) -> str:\n",
        "#     return re.sub(r'\\s*,+\\s*', ' ', s)\n",
        "\n",
        "def remove_commas(s) -> str:\n",
        "    # Logic: Replace commas with a space, BUT NOT if they are between digits (3,2)\n",
        "    # The regex checks: Is there a digit before AND after? If so, keep it.\n",
        "    return re.sub(r'(?<!\\d)\\s*,+\\s*|\\s*,+\\s*(?!\\d)', ' ', s)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "464a9889",
      "metadata": {
        "id": "464a9889",
        "lines_to_next_cell": 1
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc9a5a68",
      "metadata": {
        "id": "bc9a5a68"
      },
      "outputs": [],
      "source": [
        "def test_tokenization(model=None):\n",
        "    s = 'Vēl 9% sacīja, ka nav izlēmuši kā balsot, bet 3,2% atteicās atbildēt.'\n",
        "    if model:\n",
        "        print('Tokenizer', model)\n",
        "        t = AutoTokenizer.from_pretrained(model)\n",
        "        print('Encoded sample:', t(s))\n",
        "        print('Encoded sample - subword units:', t.convert_ids_to_tokens(t.encode(s)))\n",
        "        lengths = sorted([len(t.encode(seq)) for seq in train_texts])\n",
        "        print(f'Max {max(lengths)}, min {min(lengths)}, avg {sum(lengths)/len(lengths)}')\n",
        "        print(f'95% length: {lengths[int(len(lengths) * 0.95)]}')\n",
        "        print(f'99% length: {lengths[int(len(lengths) * 0.99)]}')\n",
        "        print(f'99.9% length: {lengths[int(len(lengths) * 0.999)]}')\n",
        "\n",
        "test_tokenization('google/mt5-small')\n",
        "test_tokenization('google/byt5-small')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c716d714",
      "metadata": {
        "id": "c716d714",
        "lines_to_next_cell": 1
      },
      "source": [
        "Tokenize and format dataset for model training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c40227d8",
      "metadata": {
        "id": "c40227d8"
      },
      "outputs": [],
      "source": [
        "def build_dataset(*, tokenizer, train_file='train.txt', dev_file='dev.txt', train_samples=None, dev_samples=None, max_length=100):\n",
        "    ds = load_dataset('text', data_files={'train': train_file, 'dev': dev_file})\n",
        "    if train_samples:\n",
        "        ds['train'] = ds['train'].take(train_samples)\n",
        "    if dev_samples:\n",
        "        ds['dev'] = ds['dev'].take(dev_samples)\n",
        "\n",
        "    def _encode_examples(batch):\n",
        "        targets = batch['text']\n",
        "        sources = [remove_commas(t) for t in targets]\n",
        "        enc_in = tokenizer(sources, max_length=max_length, truncation=True)\n",
        "        enc_out = tokenizer(text_target=targets, max_length=max_length, truncation=True)\n",
        "        enc_in['labels'] = enc_out['input_ids']\n",
        "        return enc_in\n",
        "\n",
        "    ds_encoded = ds.map(_encode_examples, batched=True, remove_columns=ds['train'].column_names)\n",
        "\n",
        "    return ds_encoded\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained('google/mt5-small')\n",
        "ds = build_dataset(tokenizer=tok, train_samples=10, dev_samples=10)\n",
        "collator = DataCollatorForSeq2Seq(tok)\n",
        "loader = DataLoader(ds['train'], batch_size=3, shuffle=False, collate_fn=collator)\n",
        "batch = next(iter(loader))\n",
        "print(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb8cc7d7",
      "metadata": {
        "id": "bb8cc7d7",
        "lines_to_next_cell": 1
      },
      "source": [
        "# Metrics for token classification.\n",
        "*F1-score* (the harmonic mean of precision and recall) specifically for the COMMA class gives a more honest view of model quality:\n",
        "  - Precision: when the model predicts COMMA, is it right?\n",
        "  - Recall: does the model catch most of the true commas?\n",
        "  - F1: balances both, penalizing if one is much lower.\n",
        "\n",
        "*Changes* - the percentage of sentences where the model introduced modifications that were **not desired**.  \n",
        "This highlights over-correction: even if the model achieves good precision/recall on commas, a high *Changes* value means it is altering sentences unnecessarily, reducing usability in practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6143feb4",
      "metadata": {
        "id": "6143feb4",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "def align_and_count_commas(hyp: str, ref: str) -> tuple[int, int, int]:\n",
        "    sm = difflib.SequenceMatcher(a=hyp, b=ref, autojunk=False)\n",
        "    tp = fp = fn = 0\n",
        "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
        "        if tag == 'equal':\n",
        "            tp += hyp[i1:i2].count(',')\n",
        "        else:\n",
        "            fp += hyp[i1:i2].count(',')\n",
        "            fn += ref[j1:j2].count(',')\n",
        "    return tp, fp, fn\n",
        "\n",
        "\n",
        "def eval_commas(refs: list[str], preds: list[str], verbose=False) -> dict[str, float]:\n",
        "    verbose_changes_limit = 5\n",
        "    tp = fp = fn = changes = exact = 0\n",
        "    for hyp, ref in zip(preds, refs):\n",
        "        tpp, fpp, fnn = align_and_count_commas(hyp, ref)\n",
        "        tp += tpp; fp += fpp; fn += fnn\n",
        "        if hyp == ref:\n",
        "            exact += 1\n",
        "        is_changed = re.sub(r'[\\s,]', '', hyp) != re.sub(r'[\\s,]', '', ref)\n",
        "        if is_changed:\n",
        "            changes += 1\n",
        "\n",
        "        if verbose and verbose_changes_limit > 0 and is_changed:\n",
        "            print('--- Changed')\n",
        "            print('REF:', ref)\n",
        "            print('OUT:', hyp)\n",
        "            verbose_changes_limit -= 1\n",
        "\n",
        "    p = tp / (tp + fp) if (tp + fp) else 0.0\n",
        "    r = tp / (tp + fn) if (tp + fn) else 0.0\n",
        "    f1 = 2 * p * r / (p + r) if (p + r) else 0.0\n",
        "    return {\n",
        "        'f1': f1, 'p': p, 'r': r,\n",
        "        'changes': (changes / len(preds) if preds else 0.0),\n",
        "        'exact':  (exact / len(preds) if preds else 0.0),\n",
        "        'tp': tp, 'fp': fp, 'fn': fn,\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds, tokenizer, verbose=False):\n",
        "    preds, labels = eval_preds\n",
        "    pad_id = tokenizer.pad_token_id\n",
        "\n",
        "    # Replace ignore index in preds\n",
        "    labels = np.where(labels != -100, labels, pad_id)\n",
        "    preds = np.where(preds != -100, preds, pad_id)\n",
        "\n",
        "    decoded_preds  = tokenizer.batch_decode(preds,  skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    return eval_commas(decoded_labels, decoded_preds, verbose=verbose)\n",
        "\n",
        "print(eval_commas(\n",
        "    ['Labi atpūšamies, draugi mīļie, un lai veiksmīga, sportiska, panākumiem bagāta mums visiem jaunā vasaras sezona!'],\n",
        "    ['Labi atpūšamies, draugi, mīļie un lai veiksmīga, sportiska, panākumiem bagāta mums visiem jaunā vasaras sezona!'],\n",
        "))\n",
        "print(eval_commas(\n",
        "    ['Labi atpūšamies, draugi mīļie, un lai veiksmīga, sportiska, panākumiem bagāta mums visiem jaunā vasaras sezona!'],\n",
        "    ['Labi atpūšamies draugi mīļie un lai veiksmīgas, sportiska, panākumiem bagāta mums visiem jaunā vasaras sezona.'],\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fd81c8d",
      "metadata": {
        "id": "6fd81c8d",
        "lines_to_next_cell": 1
      },
      "source": [
        "# Inference\n",
        "Given plain text, we strip commas, tokenize with word boundaries, run the model, and insert commas after tokens labeled COMMA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "241e8b94",
      "metadata": {
        "id": "241e8b94"
      },
      "outputs": [],
      "source": [
        "def process_text(text, model, tokenizer: PreTrainedTokenizerBase, max_len=120, verbose=True):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    source = remove_commas(text)\n",
        "    inputs = tokenizer([source], return_tensors='pt', truncation=True, max_length=max_len).to(device)\n",
        "    with torch.no_grad():\n",
        "        gen = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_len,\n",
        "        )\n",
        "    result = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
        "    if verbose:\n",
        "        print(f'REF: {text}')\n",
        "        print(f' IN: {source}')\n",
        "        print(f'OUT: {result}')\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "400d8b3a",
      "metadata": {
        "id": "400d8b3a",
        "lines_to_next_cell": 1
      },
      "source": [
        "# Model fine-tuning\n",
        "- Track loss curves, gradient norms, and evaluation metrics over time\n",
        "- Use an appropriate optimizer and learning rate schedule (e.g., warmup + decay)\n",
        "- Watch for overfitting (gap between train and eval performance)\n",
        "- Adjust batch size, accumulation steps, or precision (fp16/bf16) if needed\n",
        "- Save best checkpoints based on validation metric (e.g., F1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02b22c26",
      "metadata": {
        "id": "02b22c26"
      },
      "outputs": [],
      "source": [
        "def main(\n",
        "    name='punctuator',\n",
        "    base_model='google/mt5-small',\n",
        "    max_len=80,\n",
        "    seed=42,\n",
        "    verbose=True,\n",
        "    lr=1e-3,\n",
        "    bs=8,\n",
        "    train_samples=None,\n",
        "    dev_samples=100,\n",
        "    epochs=3,\n",
        "    report_wandb=True,\n",
        "    wandb_group=None\n",
        "):\n",
        "    if report_wandb and not wandb.api.api_key:\n",
        "        print('Not authenticated with W&B')\n",
        "        report_wandb = False\n",
        "\n",
        "    with wandb.init(project='punctuator', group=wandb_group, name=name) if report_wandb else nullcontext():\n",
        "        print('Train:', locals())\n",
        "        set_seed(seed)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "\n",
        "        # Load dataset\n",
        "        ds = build_dataset(tokenizer=tokenizer, train_samples=train_samples, dev_samples=dev_samples, max_length=max_len)\n",
        "\n",
        "        # Initialize base model for tokenize sequence to sequence task\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(base_model)\n",
        "        model.config.use_cache = False\n",
        "\n",
        "        # Define training hyperparameters\n",
        "        training_args = Seq2SeqTrainingArguments(\n",
        "            output_dir=name,\n",
        "            report_to='wandb' if report_wandb else 'none',\n",
        "            learning_rate=lr,\n",
        "            per_device_train_batch_size=bs,\n",
        "            per_device_eval_batch_size=bs,\n",
        "            num_train_epochs=epochs,\n",
        "            warmup_ratio=0.05,\n",
        "            gradient_accumulation_steps=1,\n",
        "            gradient_checkpointing=True,\n",
        "            bf16=True,\n",
        "            logging_steps=50,\n",
        "            save_total_limit=1,\n",
        "            save_strategy='epoch',\n",
        "            eval_strategy='epoch',\n",
        "            eval_accumulation_steps=1,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model='f1',\n",
        "            greater_is_better=True,\n",
        "            predict_with_generate=True,\n",
        "            generation_max_length=max_len * 2,\n",
        "        )\n",
        "\n",
        "        trainer = Seq2SeqTrainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=ds['train'],\n",
        "            eval_dataset=ds['dev'],\n",
        "            processing_class=tokenizer,\n",
        "            data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
        "            compute_metrics=lambda p: compute_metrics(p, tokenizer, verbose=verbose),\n",
        "        )\n",
        "\n",
        "        # Actual training\n",
        "        trainer.train()\n",
        "        trainer.save_model(name)\n",
        "        tokenizer.save_pretrained(name)\n",
        "\n",
        "        process_text('Vēl 9% sacīja, ka nav izlēmuši kā balsot, bet 3,2% atteicās atbildēt.', trainer.model, tokenizer, max_len=max_len)\n",
        "\n",
        "main('mt5_punctuator_sample', train_samples=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a996926c",
      "metadata": {
        "id": "a996926c"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83fe76c7",
      "metadata": {
        "id": "83fe76c7",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "m = AutoModelForSeq2SeqLM.from_pretrained('mt5_punctuator_sample')\n",
        "t = AutoTokenizer.from_pretrained('mt5_punctuator_sample')\n",
        "process_text('Nogalināt nedrīkst, apžēlot!', m, t)\n",
        "process_text('Palielināt izdevumus nedrīkst taupīt!', m, t)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}